{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118 cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (65.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yousef\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "eng = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was retrieved from [kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from nltk.stem.snowball  import SnowballStemmer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "FILE_PATH = \"Sentiment140/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# create our DataPipe which is an iterable of filenames\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "\n",
    "# open the file and parse the CSV; returns an iterable of tuples\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter=\",\", as_tuple=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', '1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY', '_TheSpecialOne_', \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\")\n"
     ]
    }
   ],
   "source": [
    "# what does a row look like?\n",
    "for row in data_pipe:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only be working on the first & last columns\n",
    "# i.e. the (text, label) \n",
    "data_pipe = data_pipe.map(lambda row: (row[len(row) - 1], row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our preprocessing will consist of stemming the word\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def get_preprocessed_tokens(text, preprocess = lambda word: stemmer.stem(word)):\n",
    "    \"\"\"\n",
    "    Tokenize a text & preprocess each token using preprocess (function) \n",
    "    and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [preprocess(token.text) for token in eng.tokenizer(text)]\n",
    "\n",
    "def preprocessed_tokens_iter(data_pipe, preprocess = lambda word: stemmer.stem(word)):\n",
    "    for text, _ in data_pipe:\n",
    "        yield get_preprocessed_tokens(text, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', 'i', '!', '.', ' ', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "# build our vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    preprocessed_tokens_iter(data_pipe),\n",
    "    min_freq=2,\n",
    "\n",
    "    # special tokens include passing, start & end of sentence, & unknown\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# set the default token; if word is not found, use <unk>\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tranform = T.Sequential(\n",
    "    # converts the sequence to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    \n",
    "    # add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 because index for <sos> in vocab\n",
    "    T.AddToken(1, begin=True),\n",
    "\n",
    "    # add <eos> at beginning of each sentence\n",
    "    # 2 because index for <eos> in vocab\n",
    "    T.AddToken(2, begin=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:                 0\n",
      "Original Sequence:     @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Preprocessed Sequence: <sos> @switchfoot <unk> - awww , that 's a bummer .   you shoulda got david carr of third day to do it . ;d <eos>\n",
      "Transformed Sequence:  [1, 28448, 3, 34, 498, 10, 23, 22, 11, 1197, 6, 7, 15, 3250, 56, 866, 8024, 21, 1837, 41, 8, 31, 13, 6, 2009, 2]\n"
     ]
    }
   ],
   "source": [
    "# what does our data look like?\n",
    "for original_sequence, label in data_pipe:\n",
    "    itos = vocab.get_itos()\n",
    "    transformed_sequence = text_tranform(get_preprocessed_tokens(original_sequence))\n",
    "    preprocessed_sequence = [itos[i] for i in transformed_sequence]\n",
    "    print(f\"Label:                 {label}\")\n",
    "    print(f\"Original Sequence:     {original_sequence}\")\n",
    "    print(f\"Preprocessed Sequence: {' '.join(preprocessed_sequence)}\")\n",
    "    print(f\"Transformed Sequence:  {transformed_sequence}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 28448, 3, 34, 498, 10, 23, 22, 11, 1197, 6, 7, 15, 3250, 56, 866, 8024, 21, 1837, 41, 8, 31, 13, 6, 2009, 2], 0)\n"
     ]
    }
   ],
   "source": [
    "# apply our transformation to our pipe\n",
    "def apply_transform(row):\n",
    "    text, label = row\n",
    "    return (\n",
    "        text_tranform(get_preprocessed_tokens(text)), \n",
    "        0 if int(label) == 0 else 1\n",
    "    )\n",
    "data_pipe = data_pipe.map(apply_transform)\n",
    "\n",
    "# what does a row look like?\n",
    "for row in data_pipe:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 1635, 7, 14, 8343, 44, 7347, 2], 0), ([1, 211, 21, 44, 3778, 5, 4, 97, 11, 459, 6, 180, 25, 350, 6, 7, 371, 1112, 6, 4, 43, 309, 3, 6, 7, 14, 12, 189753, 3, 2], 1), ([1, 15, 72, 63, 4, 83, 53, 81, 55, 69, 509, 217, 103, 17, 7, 636, 1414, 5, 7, 193, 715, 4, 76, 23, 17, 7, 136, 2], 1), ([1, 92083, 563, 17, 5, 194, 917, 10, 4, 73, 2], 1), ([1, 33, 56, 68, 71, 3, 7, 106, 414, 8, 11, 202, 184, 5, 2], 1), ([1, 50, 4, 67, 16, 8, 29, 103, 10, 375, 1652, 43, 102, 46, 1297, 10, 14, 1461, 87, 39776, 63, 4, 125, 27, 125, 8, 31, 16, 40, 44, 424, 775, 268, 2], 0), ([1, 110765, 4, 33, 85, 23, 1162, 35, 5000, 532, 18, 2], 0), ([1, 50, 44, 233, 81, 41107, 14, 24097, 22, 693, 16, 98, 26, 6273, 21, 417, 14, 3, 22, 693, 537, 34, 1850, 2043, 513, 2], 1), ([1, 4, 51, 44, 466, 3045, 13, 22, 388, 7, 136, 2], 1), ([1, 35, 12, 1699, 9465, 10, 887, 2360, 3, 115, 84, 46, 277, 5, 810, 2], 1), ([1, 61789, 31480, 9764, 86918, 2], 0), ([1, 2189, 10, 4, 75, 65, 15, 183, 42, 1854, 42, 28, 65, 15, 2], 1), ([1, 3, 173, 42, 23, 333, 222, 42, 2], 0), ([1, 145383, 969, 978, 10, 88, 54, 15, 17, 2], 1), ([1, 661, 48, 462, 10, 2392, 1726, 10, 11, 1520, 1560, 21, 741, 2273, 14, 11, 11119, 14111, 19, 553, 6, 6, 6, 23, 176, 88, 4, 980, 2], 1), ([1, 151944, 2008, 104, 49, 87, 1039, 75, 72, 199, 223, 1670, 2], 0), ([1, 28016, 359, 1015, 31, 64, 10, 28, 173, 18, 55, 3890, 330, 127, 6, 2], 0), ([1, 30079, 146, 31, 15, 170, 60, 466, 17, 5, 17, 5, 17, 2], 0), ([1, 33, 45, 535, 590, 6, 130, 1099, 48, 10, 191, 18, 20051, 6, 2], 0), ([1, 268, 1449, 6, 58, 8, 29, 15382, 5, 18, 515, 25, 1535, 208, 21, 2487, 10, 974, 677, 8, 332, 449, 145, 41, 5, 7, 757, 953, 3265, 337, 6, 2], 1), ([1, 80493, 505, 2], 1), ([1, 4, 31, 24, 51, 366, 46676, 6, 28, 4, 79, 60, 484, 6, 7, 2], 0), ([1, 64675, 7, 4, 67, 8, 29, 8, 177, 458, 228, 19, 12, 257, 118, 118, 118, 118, 2], 0), ([1, 4, 38, 33, 80, 88, 202, 92, 130, 341, 4, 27, 1864, 37, 11, 1867, 531, 6, 114, 4, 73, 10, 243, 531, 7, 4475, 16, 326, 353, 569, 9, 374, 2], 0), ([1, 199406, 34, 156, 34, 25644, 7591, 7, 3, 6, 10470, 292, 191407, 135, 473, 114, 569, 423, 5, 15465, 394, 1374, 40865, 5, 2], 1), ([1, 38395, 4, 2502, 88, 116, 4, 53, 1729, 30, 156, 105, 2877, 105, 22, 4393, 10, 1553, 70, 2688, 165, 118, 542, 1553, 473, 2041, 45, 5, 2], 1), ([1, 95, 5535, 14864, 131, 1493, 1583, 239, 9, 295, 295, 180, 2], 1), ([1, 161259, 139, 22, 11, 582, 177, 12030, 6, 7, 57, 38, 368, 206, 630, 6, 2], 1), ([1, 6199, 7, 299, 5, 2007, 15, 123, 9, 3, 5, 5, 5, 4, 53, 15, 5, 3356, 4, 51, 28, 116, 88, 15, 527, 5, 204, 150, 26, 2], 0), ([1, 50, 154, 54, 19, 3, 2], 0), ([1, 544, 4, 33, 80, 51, 96, 31, 24, 97, 26, 446, 14, 4, 70, 217, 97, 6, 7, 4, 67, 12, 2982, 68, 2], 0), ([1, 19, 100, 1501, 618, 14, 479, 813, 52, 7, 66, 7, 2961, 1113, 88927, 2], 0)]\n"
     ]
    }
   ],
   "source": [
    "batch_data_pipe = data_pipe.shuffle(buffer_size=1600000).bucketbatch(\n",
    "    batch_size=32,\n",
    "    batch_num=1,\n",
    "    bucket_num=100,\n",
    "    use_in_batch_shuffle=False,\n",
    ")\n",
    "\n",
    "# what does a row look like?\n",
    "for row in batch_data_pipe:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\notebook.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m batch_padded_data_pipe \u001b[39m=\u001b[39m batch_data_pipe\u001b[39m.\u001b[39mmap(apply_transform)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# what does a row look like?\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m batch_data_pipe:\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(row)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:183\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\grouping.py:70\u001b[0m, in \u001b[0;36mBatcherIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[DataChunk]:\n\u001b[0;32m     69\u001b[0m     batch: List \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m     71\u001b[0m         batch\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m     72\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:183\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combinatorics.py:126\u001b[0m, in \u001b[0;36mShufflerIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m    127\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size:\n\u001b[0;32m    128\u001b[0m             idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:183\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combinatorics.py:126\u001b[0m, in \u001b[0;36mShufflerIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m    127\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size:\n\u001b[0;32m    128\u001b[0m             idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:195\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    194\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 195\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    196\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[T_co]:\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m    123\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:195\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    194\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 195\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    196\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[T_co]:\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[0;32m    123\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:195\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[39;00m\n\u001b[0;32m    194\u001b[0m             _check_iterator_valid(datapipe, iterator_id)\n\u001b[1;32m--> 195\u001b[0m             response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m    196\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:173\u001b[0m, in \u001b[0;36m_CSVBaseParserIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csv_reader(stream, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmtparams)\n\u001b[0;32m    172\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_helper\u001b[39m.\u001b[39mas_tuple(stream)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_helper\u001b[39m.\u001b[39mreturn_path(stream, path\u001b[39m=\u001b[39mpath)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:69\u001b[0m, in \u001b[0;36mPlainTextReaderHelper.return_path\u001b[1;34m(self, stream, path)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreturn_path\u001b[39m(\u001b[39mself\u001b[39m, stream: Iterator[D], \u001b[39m*\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Union[D, Tuple[\u001b[39mstr\u001b[39m, D]]]:\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_path:\n\u001b[1;32m---> 69\u001b[0m         \u001b[39myield from\u001b[39;00m stream\n\u001b[0;32m     70\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:78\u001b[0m, in \u001b[0;36mPlainTextReaderHelper.as_tuple\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39myield from\u001b[39;00m stream\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m stream:\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[0;32m     80\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mtuple\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:65\u001b[0m, in \u001b[0;36mPlainTextReaderHelper.decode\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m---> 65\u001b[0m         \u001b[39myield\u001b[39;00m line\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_errors) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(line, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m line\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def apply_paddings(row):\n",
    "    text, label = row\n",
    "    return (T.ToTensor(0)(text), label)\n",
    "batch_padded_data_pipe = batch_data_pipe.map(apply_transform)\n",
    "\n",
    "# what does a row look like?\n",
    "for row in batch_data_pipe:\n",
    "    print(row)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input type not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\notebook.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m T\u001b[39m.\u001b[39;49mToTensor(\u001b[39m0\u001b[39;49m)([\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    132\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    :param input: Sequence or batch of token ids\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    :type input: Union[List[int], List[List[int]]]\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m    :rtype: Tensor\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(\u001b[39minput\u001b[39;49m, padding_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_value, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\functional.py:38\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(input, padding_value, dtype)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput type not supported\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Input type not supported"
     ]
    }
   ],
   "source": [
    "T.ToTensor(0)([\"test\",\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative examples: 491\n",
      "positive examples: 509\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(FILE_PATH, header=None, encoding='latin-1')\n",
    "header_names = [\"label\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df.columns = header_names\n",
    "\n",
    "# the dataset actually only contains labels with 0 and 4\n",
    "# this will be simplified with 0 for negative and 1 for positive\n",
    "df['label'] = df['label'].apply(lambda value: 1 if value != 0 else 0)\n",
    "df = df.sample(frac=1.0, random_state=SEED).head(1000)\n",
    "\n",
    "# check the distributions\n",
    "print(f\"negative examples: {df['label'].eq(0).sum()}\")\n",
    "print(f\"positive examples: {df['label'].eq(1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data field for text (text is sequential and to lower)\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True, pad_token='<pad>', fix_length=32)\n",
    "\n",
    "# data field for label (label is a float between 0 and 1)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# create examples by iterating over DataFrame\n",
    "examples = []\n",
    "for _, row in df.iterrows():\n",
    "    # get text\n",
    "    text = row['text']\n",
    "\n",
    "     # convert to float\n",
    "    label = float(row['label'])\n",
    "    examples.append(data.Example.fromlist([text, label], fields=[('text', TEXT), ('label', LABEL)]))\n",
    "\n",
    "# split into train, validation, and test sets\n",
    "train_data, valid_data, test_data = data.Dataset(examples, fields=[('text', TEXT), ('label', LABEL)]).split(split_ratio=[0.7, 0.15, 0.15])\n",
    "\n",
    "# build the vocab (`min_freq=2` will leave out the words that appear only once)\n",
    "TEXT.build_vocab(train_data, min_freq=2)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=32,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LabelField' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\notebook.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LABEL\u001b[39m.\u001b[39;49mvocab\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelField' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LabelField' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\notebook.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# what does the data look like?\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_iter:\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     text \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mtext\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     label \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mlabel\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m             minibatch\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_key, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 156\u001b[0m     \u001b[39myield\u001b[39;00m Batch(minibatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\data\\batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     batch \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(x, name) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m---> 34\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, field\u001b[39m.\u001b[39;49mprocess(batch, device\u001b[39m=\u001b[39;49mdevice))\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \n\u001b[0;32m    228\u001b[0m \u001b[39mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m padded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(batch)\n\u001b[1;32m--> 237\u001b[0m tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumericalize(padded, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:338\u001b[0m, in \u001b[0;36mField.numericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:338\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelField' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# what does the data look like?\n",
    "for batch in train_iter:\n",
    "    text = batch.text\n",
    "    label = batch.label\n",
    "    \n",
    "    # take the first embedding in the batch\n",
    "    text_vector = text.tolist()[0] \n",
    "    \n",
    "    # take the first label in the batch \n",
    "    label_vector = label.tolist()[0]\n",
    "    \n",
    "    # convert embedding back to words\n",
    "    text_words = ' '.join([TEXT.vocab.itos[i] for i in text_vector])\n",
    "    \n",
    "    # print the vectorized text and label for the first example in the batch\n",
    "    print(f\"text_vector = {text_vector}\")\n",
    "    print(f\"text_words = \\\"{text_words}\\\"\")\n",
    "    print(f\"label = {label_vector}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 73.18%, Validation Accuracy: 51.33%\n",
      "Epoch 2, Average Loss: 69.84%, Validation Accuracy: 51.33%\n",
      "Epoch 3, Average Loss: 69.32%, Validation Accuracy: 51.33%\n",
      "Epoch 4, Average Loss: 69.69%, Validation Accuracy: 52.00%\n",
      "Epoch 5, Average Loss: 69.42%, Validation Accuracy: 51.33%\n",
      "Epoch 6, Average Loss: 69.94%, Validation Accuracy: 51.33%\n",
      "Epoch 7, Average Loss: 71.93%, Validation Accuracy: 52.00%\n",
      "Epoch 8, Average Loss: 71.17%, Validation Accuracy: 51.33%\n",
      "Epoch 9, Average Loss: 70.15%, Validation Accuracy: 51.33%\n",
      "Epoch 10, Average Loss: 69.46%, Validation Accuracy: 48.67%\n",
      "Epoch 11, Average Loss: 69.73%, Validation Accuracy: 51.33%\n",
      "Epoch 12, Average Loss: 69.73%, Validation Accuracy: 51.33%\n",
      "Epoch 13, Average Loss: 69.62%, Validation Accuracy: 48.67%\n",
      "Epoch 14, Average Loss: 69.22%, Validation Accuracy: 51.33%\n",
      "Epoch 15, Average Loss: 69.50%, Validation Accuracy: 48.67%\n",
      "Epoch 16, Average Loss: 69.08%, Validation Accuracy: 51.33%\n",
      "Epoch 17, Average Loss: 69.36%, Validation Accuracy: 48.00%\n",
      "Epoch 18, Average Loss: 69.25%, Validation Accuracy: 48.00%\n",
      "Epoch 19, Average Loss: 69.20%, Validation Accuracy: 52.00%\n",
      "Epoch 20, Average Loss: 69.17%, Validation Accuracy: 48.00%\n",
      "Epoch 21, Average Loss: 69.34%, Validation Accuracy: 48.00%\n",
      "Epoch 22, Average Loss: 69.62%, Validation Accuracy: 51.33%\n",
      "Epoch 23, Average Loss: 69.43%, Validation Accuracy: 47.33%\n",
      "Epoch 24, Average Loss: 68.78%, Validation Accuracy: 50.67%\n",
      "Epoch 25, Average Loss: 68.37%, Validation Accuracy: 51.33%\n",
      "Epoch 26, Average Loss: 69.29%, Validation Accuracy: 52.67%\n",
      "Epoch 27, Average Loss: 69.11%, Validation Accuracy: 51.33%\n",
      "Epoch 28, Average Loss: 69.42%, Validation Accuracy: 51.33%\n",
      "Epoch 29, Average Loss: 69.52%, Validation Accuracy: 48.00%\n",
      "Epoch 30, Average Loss: 69.50%, Validation Accuracy: 51.33%\n",
      "Epoch 31, Average Loss: 69.64%, Validation Accuracy: 51.33%\n",
      "Epoch 32, Average Loss: 69.05%, Validation Accuracy: 48.00%\n",
      "Epoch 33, Average Loss: 69.43%, Validation Accuracy: 51.33%\n",
      "Epoch 34, Average Loss: 69.98%, Validation Accuracy: 48.00%\n",
      "Epoch 35, Average Loss: 69.31%, Validation Accuracy: 45.33%\n",
      "Epoch 36, Average Loss: 69.46%, Validation Accuracy: 48.00%\n",
      "Epoch 37, Average Loss: 68.02%, Validation Accuracy: 50.67%\n",
      "Epoch 38, Average Loss: 68.46%, Validation Accuracy: 54.00%\n",
      "Epoch 39, Average Loss: 66.54%, Validation Accuracy: 48.67%\n",
      "Epoch 40, Average Loss: 65.22%, Validation Accuracy: 46.00%\n",
      "Epoch 41, Average Loss: 66.62%, Validation Accuracy: 45.33%\n",
      "Epoch 42, Average Loss: 64.27%, Validation Accuracy: 48.67%\n",
      "Epoch 43, Average Loss: 63.92%, Validation Accuracy: 48.00%\n",
      "Epoch 44, Average Loss: 67.03%, Validation Accuracy: 48.67%\n",
      "Epoch 45, Average Loss: 68.60%, Validation Accuracy: 52.67%\n",
      "Epoch 46, Average Loss: 67.53%, Validation Accuracy: 52.00%\n",
      "Epoch 47, Average Loss: 64.52%, Validation Accuracy: 48.00%\n",
      "Epoch 48, Average Loss: 68.03%, Validation Accuracy: 47.33%\n",
      "Epoch 49, Average Loss: 63.42%, Validation Accuracy: 50.67%\n",
      "Epoch 50, Average Loss: 64.39%, Validation Accuracy: 48.00%\n",
      "Epoch 51, Average Loss: 67.22%, Validation Accuracy: 53.33%\n",
      "Epoch 52, Average Loss: 68.52%, Validation Accuracy: 54.67%\n",
      "Epoch 53, Average Loss: 65.87%, Validation Accuracy: 54.00%\n",
      "Epoch 54, Average Loss: 67.44%, Validation Accuracy: 47.33%\n",
      "Epoch 55, Average Loss: 66.17%, Validation Accuracy: 46.67%\n",
      "Epoch 56, Average Loss: 61.30%, Validation Accuracy: 55.33%\n",
      "Epoch 57, Average Loss: 63.18%, Validation Accuracy: 54.00%\n",
      "Epoch 58, Average Loss: 66.90%, Validation Accuracy: 48.00%\n",
      "Epoch 59, Average Loss: 65.80%, Validation Accuracy: 54.00%\n",
      "Epoch 60, Average Loss: 63.38%, Validation Accuracy: 48.67%\n",
      "Epoch 61, Average Loss: 63.31%, Validation Accuracy: 53.33%\n",
      "Epoch 62, Average Loss: 60.57%, Validation Accuracy: 50.67%\n",
      "Epoch 63, Average Loss: 59.18%, Validation Accuracy: 54.00%\n",
      "Epoch 64, Average Loss: 58.17%, Validation Accuracy: 54.00%\n",
      "Epoch 65, Average Loss: 60.27%, Validation Accuracy: 48.00%\n",
      "Epoch 66, Average Loss: 66.45%, Validation Accuracy: 53.33%\n",
      "Epoch 67, Average Loss: 67.55%, Validation Accuracy: 52.67%\n",
      "Epoch 68, Average Loss: 65.58%, Validation Accuracy: 52.67%\n",
      "Epoch 69, Average Loss: 64.01%, Validation Accuracy: 50.67%\n",
      "Epoch 70, Average Loss: 61.37%, Validation Accuracy: 51.33%\n",
      "Epoch 71, Average Loss: 58.02%, Validation Accuracy: 49.33%\n",
      "Epoch 72, Average Loss: 56.48%, Validation Accuracy: 49.33%\n",
      "Epoch 73, Average Loss: 69.74%, Validation Accuracy: 53.33%\n",
      "Epoch 74, Average Loss: 68.16%, Validation Accuracy: 56.67%\n",
      "Epoch 75, Average Loss: 65.54%, Validation Accuracy: 55.33%\n",
      "Epoch 76, Average Loss: 64.23%, Validation Accuracy: 54.00%\n",
      "Epoch 77, Average Loss: 61.57%, Validation Accuracy: 54.00%\n",
      "Epoch 78, Average Loss: 62.34%, Validation Accuracy: 55.33%\n",
      "Epoch 79, Average Loss: 59.92%, Validation Accuracy: 50.67%\n",
      "Epoch 80, Average Loss: 58.10%, Validation Accuracy: 56.00%\n",
      "Epoch 81, Average Loss: 59.05%, Validation Accuracy: 51.33%\n",
      "Epoch 82, Average Loss: 58.71%, Validation Accuracy: 54.67%\n",
      "Epoch 83, Average Loss: 63.94%, Validation Accuracy: 46.67%\n",
      "Epoch 84, Average Loss: 62.62%, Validation Accuracy: 44.67%\n",
      "Epoch 85, Average Loss: 62.64%, Validation Accuracy: 52.00%\n",
      "Epoch 86, Average Loss: 64.40%, Validation Accuracy: 52.67%\n",
      "Epoch 87, Average Loss: 62.49%, Validation Accuracy: 49.33%\n",
      "Epoch 88, Average Loss: 62.34%, Validation Accuracy: 50.67%\n",
      "Epoch 89, Average Loss: 58.45%, Validation Accuracy: 49.33%\n",
      "Epoch 90, Average Loss: 57.25%, Validation Accuracy: 51.33%\n",
      "Epoch 91, Average Loss: 56.75%, Validation Accuracy: 50.00%\n",
      "Epoch 92, Average Loss: 55.73%, Validation Accuracy: 46.67%\n",
      "Epoch 93, Average Loss: 55.47%, Validation Accuracy: 46.67%\n",
      "Epoch 94, Average Loss: 55.22%, Validation Accuracy: 47.33%\n",
      "Epoch 95, Average Loss: 56.89%, Validation Accuracy: 48.00%\n",
      "Epoch 96, Average Loss: 59.67%, Validation Accuracy: 48.67%\n",
      "Epoch 97, Average Loss: 54.52%, Validation Accuracy: 46.67%\n",
      "Epoch 98, Average Loss: 54.05%, Validation Accuracy: 46.67%\n",
      "Epoch 99, Average Loss: 53.90%, Validation Accuracy: 47.33%\n",
      "Epoch 100, Average Loss: 52.18%, Validation Accuracy: 49.33%\n",
      "Epoch 101, Average Loss: 53.92%, Validation Accuracy: 46.00%\n",
      "Epoch 102, Average Loss: 53.53%, Validation Accuracy: 46.00%\n",
      "Epoch 103, Average Loss: 54.24%, Validation Accuracy: 45.33%\n",
      "Epoch 104, Average Loss: 53.37%, Validation Accuracy: 46.00%\n",
      "Epoch 105, Average Loss: 52.28%, Validation Accuracy: 46.67%\n",
      "Epoch 106, Average Loss: 62.69%, Validation Accuracy: 50.67%\n",
      "Epoch 107, Average Loss: 68.50%, Validation Accuracy: 46.00%\n",
      "Epoch 108, Average Loss: 68.74%, Validation Accuracy: 52.67%\n",
      "Epoch 109, Average Loss: 67.57%, Validation Accuracy: 47.33%\n",
      "Epoch 110, Average Loss: 67.64%, Validation Accuracy: 45.33%\n",
      "Epoch 111, Average Loss: 65.77%, Validation Accuracy: 47.33%\n",
      "Epoch 112, Average Loss: 63.13%, Validation Accuracy: 48.67%\n",
      "Epoch 113, Average Loss: 60.28%, Validation Accuracy: 50.00%\n",
      "Epoch 114, Average Loss: 56.74%, Validation Accuracy: 50.67%\n",
      "Epoch 115, Average Loss: 60.34%, Validation Accuracy: 50.00%\n",
      "Epoch 116, Average Loss: 62.86%, Validation Accuracy: 49.33%\n",
      "Epoch 117, Average Loss: 58.50%, Validation Accuracy: 52.67%\n",
      "Epoch 118, Average Loss: 56.16%, Validation Accuracy: 56.67%\n",
      "Epoch 119, Average Loss: 54.57%, Validation Accuracy: 50.00%\n",
      "Epoch 120, Average Loss: 55.98%, Validation Accuracy: 49.33%\n",
      "Epoch 121, Average Loss: 51.86%, Validation Accuracy: 55.33%\n",
      "Epoch 122, Average Loss: 51.35%, Validation Accuracy: 53.33%\n",
      "Epoch 123, Average Loss: 51.96%, Validation Accuracy: 52.67%\n",
      "Epoch 124, Average Loss: 52.27%, Validation Accuracy: 51.33%\n",
      "Epoch 125, Average Loss: 52.17%, Validation Accuracy: 52.67%\n",
      "Epoch 126, Average Loss: 50.77%, Validation Accuracy: 53.33%\n",
      "Epoch 127, Average Loss: 51.02%, Validation Accuracy: 52.67%\n",
      "Epoch 128, Average Loss: 50.75%, Validation Accuracy: 54.67%\n",
      "Epoch 129, Average Loss: 52.54%, Validation Accuracy: 52.00%\n",
      "Epoch 130, Average Loss: 50.73%, Validation Accuracy: 50.67%\n",
      "Epoch 131, Average Loss: 50.70%, Validation Accuracy: 50.67%\n",
      "Epoch 132, Average Loss: 52.17%, Validation Accuracy: 53.33%\n",
      "Epoch 133, Average Loss: 58.31%, Validation Accuracy: 51.33%\n",
      "Epoch 134, Average Loss: 56.28%, Validation Accuracy: 50.67%\n",
      "Epoch 135, Average Loss: 55.09%, Validation Accuracy: 52.67%\n",
      "Epoch 136, Average Loss: 57.83%, Validation Accuracy: 50.67%\n",
      "Epoch 137, Average Loss: 58.29%, Validation Accuracy: 53.33%\n",
      "Epoch 138, Average Loss: 55.63%, Validation Accuracy: 54.00%\n",
      "Epoch 139, Average Loss: 55.31%, Validation Accuracy: 54.67%\n",
      "Epoch 140, Average Loss: 53.37%, Validation Accuracy: 48.00%\n",
      "Epoch 141, Average Loss: 67.01%, Validation Accuracy: 49.33%\n",
      "Epoch 142, Average Loss: 64.40%, Validation Accuracy: 48.67%\n",
      "Epoch 143, Average Loss: 62.59%, Validation Accuracy: 47.33%\n",
      "Epoch 144, Average Loss: 60.14%, Validation Accuracy: 48.00%\n",
      "Epoch 145, Average Loss: 56.15%, Validation Accuracy: 52.00%\n",
      "Epoch 146, Average Loss: 52.20%, Validation Accuracy: 50.00%\n",
      "Epoch 147, Average Loss: 51.51%, Validation Accuracy: 51.33%\n",
      "Epoch 148, Average Loss: 55.01%, Validation Accuracy: 48.67%\n",
      "Epoch 149, Average Loss: 57.76%, Validation Accuracy: 49.33%\n",
      "Epoch 150, Average Loss: 58.40%, Validation Accuracy: 50.00%\n",
      "Epoch 151, Average Loss: 58.54%, Validation Accuracy: 51.33%\n",
      "Epoch 152, Average Loss: 55.22%, Validation Accuracy: 49.33%\n",
      "Epoch 153, Average Loss: 54.06%, Validation Accuracy: 52.00%\n",
      "Epoch 154, Average Loss: 52.92%, Validation Accuracy: 50.00%\n",
      "Epoch 155, Average Loss: 55.61%, Validation Accuracy: 50.00%\n",
      "Epoch 156, Average Loss: 54.92%, Validation Accuracy: 52.00%\n",
      "Epoch 157, Average Loss: 59.38%, Validation Accuracy: 50.00%\n",
      "Epoch 158, Average Loss: 61.96%, Validation Accuracy: 46.00%\n",
      "Epoch 159, Average Loss: 60.94%, Validation Accuracy: 54.67%\n",
      "Epoch 160, Average Loss: 56.42%, Validation Accuracy: 55.33%\n",
      "Epoch 161, Average Loss: 55.51%, Validation Accuracy: 50.00%\n",
      "Epoch 162, Average Loss: 54.65%, Validation Accuracy: 49.33%\n",
      "Epoch 163, Average Loss: 55.25%, Validation Accuracy: 49.33%\n",
      "Epoch 164, Average Loss: 53.47%, Validation Accuracy: 49.33%\n",
      "Epoch 165, Average Loss: 52.76%, Validation Accuracy: 53.33%\n",
      "Epoch 166, Average Loss: 54.10%, Validation Accuracy: 51.33%\n",
      "Epoch 167, Average Loss: 52.61%, Validation Accuracy: 52.00%\n",
      "Epoch 168, Average Loss: 68.74%, Validation Accuracy: 48.67%\n",
      "Epoch 169, Average Loss: 69.67%, Validation Accuracy: 48.00%\n",
      "Epoch 170, Average Loss: 69.47%, Validation Accuracy: 51.33%\n",
      "Epoch 171, Average Loss: 69.10%, Validation Accuracy: 48.00%\n",
      "Epoch 172, Average Loss: 69.67%, Validation Accuracy: 51.33%\n",
      "Epoch 173, Average Loss: 69.32%, Validation Accuracy: 48.00%\n",
      "Epoch 174, Average Loss: 69.36%, Validation Accuracy: 48.00%\n",
      "Epoch 175, Average Loss: 69.35%, Validation Accuracy: 48.00%\n",
      "Epoch 176, Average Loss: 69.31%, Validation Accuracy: 48.00%\n",
      "Epoch 177, Average Loss: 69.54%, Validation Accuracy: 51.33%\n",
      "Epoch 178, Average Loss: 69.11%, Validation Accuracy: 48.00%\n",
      "Epoch 179, Average Loss: 69.36%, Validation Accuracy: 51.33%\n",
      "Epoch 180, Average Loss: 69.62%, Validation Accuracy: 51.33%\n",
      "Epoch 181, Average Loss: 69.39%, Validation Accuracy: 48.00%\n",
      "Epoch 182, Average Loss: 69.69%, Validation Accuracy: 48.00%\n",
      "Epoch 183, Average Loss: 69.31%, Validation Accuracy: 48.00%\n",
      "Epoch 184, Average Loss: 69.19%, Validation Accuracy: 48.00%\n",
      "Epoch 185, Average Loss: 69.55%, Validation Accuracy: 52.00%\n",
      "Epoch 186, Average Loss: 69.12%, Validation Accuracy: 48.00%\n",
      "Epoch 187, Average Loss: 69.26%, Validation Accuracy: 51.33%\n",
      "Epoch 188, Average Loss: 69.30%, Validation Accuracy: 49.33%\n",
      "Epoch 189, Average Loss: 69.11%, Validation Accuracy: 51.33%\n",
      "Epoch 190, Average Loss: 69.18%, Validation Accuracy: 48.00%\n",
      "Epoch 191, Average Loss: 69.26%, Validation Accuracy: 51.33%\n",
      "Epoch 192, Average Loss: 69.59%, Validation Accuracy: 48.00%\n",
      "Epoch 193, Average Loss: 69.10%, Validation Accuracy: 51.33%\n",
      "Epoch 194, Average Loss: 69.33%, Validation Accuracy: 50.00%\n",
      "Epoch 195, Average Loss: 69.23%, Validation Accuracy: 51.33%\n",
      "Epoch 196, Average Loss: 69.40%, Validation Accuracy: 48.00%\n",
      "Epoch 197, Average Loss: 69.12%, Validation Accuracy: 51.33%\n",
      "Epoch 198, Average Loss: 69.31%, Validation Accuracy: 51.33%\n",
      "Epoch 199, Average Loss: 69.29%, Validation Accuracy: 48.00%\n",
      "Epoch 200, Average Loss: 69.02%, Validation Accuracy: 51.33%\n",
      "Epoch 201, Average Loss: 69.28%, Validation Accuracy: 48.00%\n",
      "Epoch 202, Average Loss: 69.02%, Validation Accuracy: 51.33%\n",
      "Epoch 203, Average Loss: 69.32%, Validation Accuracy: 48.00%\n",
      "Epoch 204, Average Loss: 69.15%, Validation Accuracy: 51.33%\n",
      "Epoch 205, Average Loss: 69.61%, Validation Accuracy: 51.33%\n",
      "Epoch 206, Average Loss: 69.14%, Validation Accuracy: 48.00%\n",
      "Epoch 207, Average Loss: 69.08%, Validation Accuracy: 50.00%\n",
      "Epoch 208, Average Loss: 69.13%, Validation Accuracy: 49.33%\n",
      "Epoch 209, Average Loss: 69.17%, Validation Accuracy: 51.33%\n",
      "Epoch 210, Average Loss: 69.12%, Validation Accuracy: 51.33%\n",
      "Epoch 211, Average Loss: 69.38%, Validation Accuracy: 51.33%\n",
      "Epoch 212, Average Loss: 69.49%, Validation Accuracy: 51.33%\n",
      "Epoch 213, Average Loss: 69.58%, Validation Accuracy: 48.00%\n",
      "Epoch 214, Average Loss: 69.09%, Validation Accuracy: 51.33%\n",
      "Epoch 215, Average Loss: 69.11%, Validation Accuracy: 50.00%\n",
      "Epoch 216, Average Loss: 69.01%, Validation Accuracy: 51.33%\n",
      "Epoch 217, Average Loss: 69.10%, Validation Accuracy: 51.33%\n",
      "Epoch 218, Average Loss: 69.08%, Validation Accuracy: 49.33%\n",
      "Epoch 219, Average Loss: 69.08%, Validation Accuracy: 51.33%\n",
      "Epoch 220, Average Loss: 69.63%, Validation Accuracy: 47.33%\n",
      "Epoch 221, Average Loss: 68.97%, Validation Accuracy: 52.00%\n",
      "Epoch 222, Average Loss: 69.13%, Validation Accuracy: 52.00%\n",
      "Epoch 223, Average Loss: 69.21%, Validation Accuracy: 48.00%\n",
      "Epoch 224, Average Loss: 69.03%, Validation Accuracy: 46.67%\n",
      "Epoch 225, Average Loss: 68.84%, Validation Accuracy: 47.33%\n",
      "Epoch 226, Average Loss: 68.84%, Validation Accuracy: 48.67%\n",
      "Epoch 227, Average Loss: 69.05%, Validation Accuracy: 46.67%\n",
      "Epoch 228, Average Loss: 68.76%, Validation Accuracy: 52.00%\n",
      "Epoch 229, Average Loss: 68.47%, Validation Accuracy: 48.00%\n",
      "Epoch 230, Average Loss: 65.70%, Validation Accuracy: 48.67%\n",
      "Epoch 231, Average Loss: 62.03%, Validation Accuracy: 49.33%\n",
      "Epoch 232, Average Loss: 65.99%, Validation Accuracy: 51.33%\n",
      "Epoch 233, Average Loss: 58.88%, Validation Accuracy: 48.67%\n",
      "Epoch 234, Average Loss: 54.73%, Validation Accuracy: 48.67%\n",
      "Epoch 235, Average Loss: 57.01%, Validation Accuracy: 46.00%\n",
      "Epoch 236, Average Loss: 53.04%, Validation Accuracy: 45.33%\n",
      "Epoch 237, Average Loss: 48.03%, Validation Accuracy: 48.67%\n",
      "Epoch 238, Average Loss: 45.63%, Validation Accuracy: 48.00%\n",
      "Epoch 239, Average Loss: 66.79%, Validation Accuracy: 46.67%\n",
      "Epoch 240, Average Loss: 67.13%, Validation Accuracy: 50.00%\n",
      "Epoch 241, Average Loss: 52.64%, Validation Accuracy: 47.33%\n",
      "Epoch 242, Average Loss: 49.06%, Validation Accuracy: 48.00%\n",
      "Epoch 243, Average Loss: 44.57%, Validation Accuracy: 46.00%\n",
      "Epoch 244, Average Loss: 42.62%, Validation Accuracy: 48.00%\n",
      "Epoch 245, Average Loss: 42.19%, Validation Accuracy: 48.00%\n",
      "Epoch 246, Average Loss: 39.56%, Validation Accuracy: 49.33%\n",
      "Epoch 247, Average Loss: 39.24%, Validation Accuracy: 46.67%\n",
      "Epoch 248, Average Loss: 37.92%, Validation Accuracy: 48.67%\n",
      "Epoch 249, Average Loss: 37.87%, Validation Accuracy: 48.00%\n",
      "Epoch 250, Average Loss: 36.61%, Validation Accuracy: 46.67%\n",
      "Epoch 251, Average Loss: 37.54%, Validation Accuracy: 48.67%\n",
      "Epoch 252, Average Loss: 35.48%, Validation Accuracy: 48.00%\n",
      "Epoch 253, Average Loss: 35.47%, Validation Accuracy: 45.33%\n",
      "Epoch 254, Average Loss: 42.52%, Validation Accuracy: 50.00%\n",
      "Epoch 255, Average Loss: 56.29%, Validation Accuracy: 49.33%\n",
      "Epoch 256, Average Loss: 50.69%, Validation Accuracy: 52.00%\n",
      "Epoch 257, Average Loss: 48.04%, Validation Accuracy: 50.67%\n",
      "Epoch 258, Average Loss: 45.37%, Validation Accuracy: 50.00%\n",
      "Epoch 259, Average Loss: 43.43%, Validation Accuracy: 48.67%\n",
      "Epoch 260, Average Loss: 45.26%, Validation Accuracy: 51.33%\n",
      "Epoch 261, Average Loss: 45.48%, Validation Accuracy: 52.00%\n",
      "Epoch 262, Average Loss: 47.53%, Validation Accuracy: 47.33%\n",
      "Epoch 263, Average Loss: 45.79%, Validation Accuracy: 47.33%\n",
      "Epoch 264, Average Loss: 52.73%, Validation Accuracy: 45.33%\n",
      "Epoch 265, Average Loss: 49.41%, Validation Accuracy: 41.33%\n",
      "Epoch 266, Average Loss: 45.17%, Validation Accuracy: 42.00%\n",
      "Epoch 267, Average Loss: 44.43%, Validation Accuracy: 46.67%\n",
      "Epoch 268, Average Loss: 43.16%, Validation Accuracy: 44.67%\n",
      "Epoch 269, Average Loss: 43.08%, Validation Accuracy: 48.67%\n",
      "Epoch 270, Average Loss: 43.32%, Validation Accuracy: 46.67%\n",
      "Epoch 271, Average Loss: 42.16%, Validation Accuracy: 47.33%\n",
      "Epoch 272, Average Loss: 41.12%, Validation Accuracy: 43.33%\n",
      "Epoch 273, Average Loss: 41.13%, Validation Accuracy: 47.33%\n",
      "Epoch 274, Average Loss: 40.42%, Validation Accuracy: 48.67%\n",
      "Epoch 275, Average Loss: 40.07%, Validation Accuracy: 48.67%\n",
      "Epoch 276, Average Loss: 40.21%, Validation Accuracy: 48.67%\n",
      "Epoch 277, Average Loss: 39.99%, Validation Accuracy: 48.67%\n",
      "Epoch 278, Average Loss: 39.79%, Validation Accuracy: 48.67%\n",
      "Epoch 279, Average Loss: 41.30%, Validation Accuracy: 46.00%\n",
      "Epoch 280, Average Loss: 43.70%, Validation Accuracy: 45.33%\n",
      "Epoch 281, Average Loss: 43.52%, Validation Accuracy: 49.33%\n",
      "Epoch 282, Average Loss: 40.75%, Validation Accuracy: 48.67%\n",
      "Epoch 283, Average Loss: 40.37%, Validation Accuracy: 48.67%\n",
      "Epoch 284, Average Loss: 40.34%, Validation Accuracy: 48.67%\n",
      "Epoch 285, Average Loss: 40.28%, Validation Accuracy: 48.67%\n",
      "Epoch 286, Average Loss: 40.55%, Validation Accuracy: 48.67%\n",
      "Epoch 287, Average Loss: 40.30%, Validation Accuracy: 49.33%\n",
      "Epoch 288, Average Loss: 40.91%, Validation Accuracy: 49.33%\n",
      "Epoch 289, Average Loss: 40.99%, Validation Accuracy: 49.33%\n",
      "Epoch 290, Average Loss: 40.87%, Validation Accuracy: 49.33%\n",
      "Epoch 291, Average Loss: 41.01%, Validation Accuracy: 49.33%\n",
      "Epoch 292, Average Loss: 40.82%, Validation Accuracy: 49.33%\n",
      "Epoch 293, Average Loss: 40.86%, Validation Accuracy: 49.33%\n",
      "Epoch 294, Average Loss: 40.81%, Validation Accuracy: 49.33%\n",
      "Epoch 295, Average Loss: 40.61%, Validation Accuracy: 49.33%\n",
      "Epoch 296, Average Loss: 40.70%, Validation Accuracy: 49.33%\n",
      "Epoch 297, Average Loss: 40.46%, Validation Accuracy: 49.33%\n",
      "Epoch 298, Average Loss: 40.64%, Validation Accuracy: 49.33%\n",
      "Epoch 299, Average Loss: 40.34%, Validation Accuracy: 48.67%\n",
      "Epoch 300, Average Loss: 40.35%, Validation Accuracy: 49.33%\n",
      "Epoch 301, Average Loss: 39.83%, Validation Accuracy: 49.33%\n",
      "Epoch 302, Average Loss: 41.58%, Validation Accuracy: 48.67%\n",
      "Epoch 303, Average Loss: 39.83%, Validation Accuracy: 48.67%\n",
      "Epoch 304, Average Loss: 39.85%, Validation Accuracy: 48.67%\n",
      "Epoch 305, Average Loss: 39.76%, Validation Accuracy: 48.67%\n",
      "Epoch 306, Average Loss: 39.80%, Validation Accuracy: 48.67%\n",
      "Epoch 307, Average Loss: 39.83%, Validation Accuracy: 48.67%\n",
      "Epoch 308, Average Loss: 39.83%, Validation Accuracy: 48.00%\n",
      "Epoch 309, Average Loss: 39.84%, Validation Accuracy: 48.00%\n",
      "Epoch 310, Average Loss: 39.87%, Validation Accuracy: 48.00%\n",
      "Epoch 311, Average Loss: 40.37%, Validation Accuracy: 48.00%\n",
      "Epoch 312, Average Loss: 40.13%, Validation Accuracy: 48.00%\n",
      "Epoch 313, Average Loss: 40.32%, Validation Accuracy: 48.00%\n",
      "Epoch 314, Average Loss: 39.95%, Validation Accuracy: 48.00%\n",
      "Epoch 315, Average Loss: 39.82%, Validation Accuracy: 48.00%\n",
      "Epoch 316, Average Loss: 39.91%, Validation Accuracy: 48.00%\n",
      "Epoch 317, Average Loss: 40.19%, Validation Accuracy: 48.00%\n",
      "Epoch 318, Average Loss: 39.87%, Validation Accuracy: 48.00%\n",
      "Epoch 319, Average Loss: 39.86%, Validation Accuracy: 48.00%\n",
      "Epoch 320, Average Loss: 39.77%, Validation Accuracy: 48.00%\n",
      "Epoch 321, Average Loss: 39.76%, Validation Accuracy: 48.00%\n",
      "Epoch 322, Average Loss: 39.87%, Validation Accuracy: 48.00%\n",
      "Epoch 323, Average Loss: 39.90%, Validation Accuracy: 47.33%\n",
      "Epoch 324, Average Loss: 39.57%, Validation Accuracy: 47.33%\n",
      "Epoch 325, Average Loss: 38.80%, Validation Accuracy: 48.00%\n",
      "Epoch 326, Average Loss: 38.86%, Validation Accuracy: 48.00%\n",
      "Epoch 327, Average Loss: 38.97%, Validation Accuracy: 48.00%\n",
      "Epoch 328, Average Loss: 38.06%, Validation Accuracy: 48.67%\n",
      "Epoch 329, Average Loss: 38.16%, Validation Accuracy: 48.67%\n",
      "Epoch 330, Average Loss: 38.16%, Validation Accuracy: 48.67%\n",
      "Epoch 331, Average Loss: 38.27%, Validation Accuracy: 48.67%\n",
      "Epoch 332, Average Loss: 38.12%, Validation Accuracy: 47.33%\n",
      "Epoch 333, Average Loss: 37.88%, Validation Accuracy: 47.33%\n",
      "Epoch 334, Average Loss: 37.90%, Validation Accuracy: 47.33%\n",
      "Epoch 335, Average Loss: 37.97%, Validation Accuracy: 47.33%\n",
      "Epoch 336, Average Loss: 37.73%, Validation Accuracy: 47.33%\n",
      "Epoch 337, Average Loss: 37.80%, Validation Accuracy: 47.33%\n",
      "Epoch 338, Average Loss: 37.74%, Validation Accuracy: 47.33%\n",
      "Epoch 339, Average Loss: 37.67%, Validation Accuracy: 47.33%\n",
      "Epoch 340, Average Loss: 37.55%, Validation Accuracy: 47.33%\n",
      "Epoch 341, Average Loss: 37.67%, Validation Accuracy: 47.33%\n",
      "Epoch 342, Average Loss: 37.85%, Validation Accuracy: 47.33%\n",
      "Epoch 343, Average Loss: 37.63%, Validation Accuracy: 47.33%\n",
      "Epoch 344, Average Loss: 37.61%, Validation Accuracy: 47.33%\n",
      "Epoch 345, Average Loss: 37.55%, Validation Accuracy: 47.33%\n",
      "Epoch 346, Average Loss: 37.77%, Validation Accuracy: 47.33%\n",
      "Epoch 347, Average Loss: 38.02%, Validation Accuracy: 47.33%\n",
      "Epoch 348, Average Loss: 37.81%, Validation Accuracy: 48.00%\n",
      "Epoch 349, Average Loss: 37.62%, Validation Accuracy: 48.00%\n",
      "Epoch 350, Average Loss: 38.03%, Validation Accuracy: 48.00%\n",
      "Epoch 351, Average Loss: 37.83%, Validation Accuracy: 48.00%\n",
      "Epoch 352, Average Loss: 37.63%, Validation Accuracy: 48.00%\n",
      "Epoch 353, Average Loss: 37.68%, Validation Accuracy: 48.00%\n",
      "Epoch 354, Average Loss: 37.84%, Validation Accuracy: 48.00%\n",
      "Epoch 355, Average Loss: 37.59%, Validation Accuracy: 48.00%\n",
      "Epoch 356, Average Loss: 37.63%, Validation Accuracy: 48.00%\n",
      "Epoch 357, Average Loss: 37.68%, Validation Accuracy: 48.67%\n",
      "Epoch 358, Average Loss: 37.68%, Validation Accuracy: 47.33%\n",
      "Epoch 359, Average Loss: 37.60%, Validation Accuracy: 48.00%\n",
      "Epoch 360, Average Loss: 37.43%, Validation Accuracy: 48.00%\n",
      "Epoch 361, Average Loss: 37.68%, Validation Accuracy: 48.00%\n",
      "Epoch 362, Average Loss: 37.85%, Validation Accuracy: 48.00%\n",
      "Epoch 363, Average Loss: 37.25%, Validation Accuracy: 48.00%\n",
      "Epoch 364, Average Loss: 37.76%, Validation Accuracy: 47.33%\n",
      "Epoch 365, Average Loss: 37.88%, Validation Accuracy: 47.33%\n",
      "Epoch 366, Average Loss: 37.66%, Validation Accuracy: 47.33%\n",
      "Epoch 367, Average Loss: 37.68%, Validation Accuracy: 47.33%\n",
      "Epoch 368, Average Loss: 37.66%, Validation Accuracy: 47.33%\n",
      "Epoch 369, Average Loss: 37.58%, Validation Accuracy: 47.33%\n",
      "Epoch 370, Average Loss: 37.66%, Validation Accuracy: 47.33%\n",
      "Epoch 371, Average Loss: 37.74%, Validation Accuracy: 47.33%\n",
      "Epoch 372, Average Loss: 37.63%, Validation Accuracy: 46.67%\n",
      "Epoch 373, Average Loss: 37.85%, Validation Accuracy: 47.33%\n",
      "Epoch 374, Average Loss: 37.71%, Validation Accuracy: 46.67%\n",
      "Epoch 375, Average Loss: 37.73%, Validation Accuracy: 47.33%\n",
      "Epoch 376, Average Loss: 37.95%, Validation Accuracy: 47.33%\n",
      "Epoch 377, Average Loss: 37.69%, Validation Accuracy: 46.67%\n",
      "Epoch 378, Average Loss: 37.89%, Validation Accuracy: 46.67%\n",
      "Epoch 379, Average Loss: 37.96%, Validation Accuracy: 46.67%\n",
      "Epoch 380, Average Loss: 38.24%, Validation Accuracy: 46.67%\n",
      "Epoch 381, Average Loss: 38.21%, Validation Accuracy: 46.67%\n",
      "Epoch 382, Average Loss: 38.07%, Validation Accuracy: 46.67%\n",
      "Epoch 383, Average Loss: 37.92%, Validation Accuracy: 46.67%\n",
      "Epoch 384, Average Loss: 37.75%, Validation Accuracy: 50.00%\n",
      "Epoch 385, Average Loss: 37.11%, Validation Accuracy: 50.00%\n",
      "Epoch 386, Average Loss: 37.16%, Validation Accuracy: 49.33%\n",
      "Epoch 387, Average Loss: 36.71%, Validation Accuracy: 49.33%\n",
      "Epoch 388, Average Loss: 36.88%, Validation Accuracy: 48.67%\n",
      "Epoch 389, Average Loss: 36.99%, Validation Accuracy: 49.33%\n",
      "Epoch 390, Average Loss: 37.10%, Validation Accuracy: 48.67%\n",
      "Epoch 391, Average Loss: 37.01%, Validation Accuracy: 48.67%\n",
      "Epoch 392, Average Loss: 36.85%, Validation Accuracy: 48.67%\n",
      "Epoch 393, Average Loss: 36.66%, Validation Accuracy: 48.67%\n",
      "Epoch 394, Average Loss: 36.99%, Validation Accuracy: 48.67%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\notebook.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/School/4-Senior-Fall/CS-577%20Deep%20Learning/assignments/Project/Sentiment-Analysis/notebook.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m RNN\u001b[39m.\u001b[39;49mtrain(model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, loss_fn\u001b[39m=\u001b[39;49mloss_fn, train_iter\u001b[39m=\u001b[39;49mtrain_iter, val_iter\u001b[39m=\u001b[39;49mval_iter, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[1;32mg:\\My Drive\\School\\4-Senior-Fall\\CS-577 Deep Learning\\assignments\\Project\\Sentiment-Analysis\\RNN.py:30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_iter, val_iter, num_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 30\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     32\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:361\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m cast(Optimizer, \u001b[39mself\u001b[39m)\n\u001b[0;32m    360\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 361\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m pre_hook \u001b[39min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_pre_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    364\u001b[0m         result \u001b[39m=\u001b[39m pre_hook(\u001b[39mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:631\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\n\u001b[0;32m    633\u001b[0m     )\n\u001b[0;32m    634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\torch\\_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import RNN\n",
    "\n",
    "model = RNN.Classifier(embedding_size=128, hidden_size=128, output_size=1, num_layers=1, vocab_size=vocab_size)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# move to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "RNN.train(model=model, optimizer=optimizer, loss_fn=loss_fn, train_iter=train_iter, val_iter=val_iter, num_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 70.59%, Validation Accuracy: 48.67%\n",
      "Epoch 2, Average Loss: 69.72%, Validation Accuracy: 51.33%\n",
      "Epoch 3, Average Loss: 69.68%, Validation Accuracy: 48.67%\n",
      "Epoch 4, Average Loss: 69.34%, Validation Accuracy: 51.33%\n",
      "Epoch 5, Average Loss: 69.40%, Validation Accuracy: 50.67%\n",
      "Epoch 6, Average Loss: 69.32%, Validation Accuracy: 51.33%\n",
      "Epoch 7, Average Loss: 69.41%, Validation Accuracy: 52.67%\n",
      "Epoch 8, Average Loss: 69.04%, Validation Accuracy: 56.67%\n",
      "Epoch 9, Average Loss: 68.38%, Validation Accuracy: 55.33%\n",
      "Epoch 10, Average Loss: 66.89%, Validation Accuracy: 52.67%\n"
     ]
    }
   ],
   "source": [
    "import LSTM\n",
    "\n",
    "model = LSTM.Classifier(embedding_size=128, hidden_size=128, output_size=1, num_layers=1, vocab_size=vocab_size)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# move to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "LSTM.train(model=model, optimizer=optimizer, loss_fn=loss_fn, train_iter=train_iter, val_iter=val_iter, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
